{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRIB to NetCDF4 conversion in the CADS\n",
    "\n",
    "This notebook demonstrates how GRIB files are converted to NetCDF in the CADS such that users have full traceability of the processing chain, and can modify the process should they have different requirements for their netCDF format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries used\n",
    "from typing import Any\n",
    "import os\n",
    "import xarray as xr\n",
    "import cfgrib\n",
    "import cdsapi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get example grib data from the CDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make it interesting we are going to request atmospheric, soil level and ocean wave data. This will give us multiple challenges in terms of handling different level types and spatial grids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"reanalysis-era5-single-levels\"\n",
    "request = {\n",
    "    'product_type': ['reanalysis'],\n",
    "    'variable': ['10m_u_component_of_wind', '10m_v_component_of_wind', '2m_dewpoint_temperature', '2m_temperature', 'mean_sea_level_pressure', 'mean_wave_direction', 'mean_wave_period', 'sea_surface_temperature', 'significant_height_of_combined_wind_waves_and_swell', 'surface_pressure', 'total_precipitation'],\n",
    "    'year': ['1985'],\n",
    "    'month': ['02'],\n",
    "    'day': ['03'],\n",
    "    'time': ['00:00', '01:00', '02:00', '03:00', '04:00', '05:00', '06:00', '07:00', '08:00', '09:00', '10:00', '11:00', '12:00', '13:00', '14:00', '15:00', '16:00', '17:00', '18:00', '19:00', '20:00', '21:00', '22:00', '23:00'],\n",
    "    'data_format': 'grib',\n",
    "    'download_format': 'unarchived'\n",
    "}\n",
    "\n",
    "grib_file = \"data.grib\"\n",
    "if not os.path.exists(grib_file): # Don't download the file if it already exists\n",
    "    client = cdsapi.Client()\n",
    "    grib_file = client.retrieve(dataset, request).download(\"data.grib\")\n",
    "\n",
    "# Get the base name of the file to use in the output file names\n",
    "fname, _ = os.path.splitext(os.path.basename(grib_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we set some options that we will use to open the grib file in xarray and perform some minor post-processing to produce our NetCDF file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting data.grib to netCDF files with:\n",
      "  open_datasets_kwargs: [{'time_dims': ['valid_time'], 'ignore_keys': ['edition'], 'extra_coords': {'expver': 'valid_time'}, 'coords_as_attributes': ['surface', 'depthBelowLandLayer', 'entireAtmosphere', 'heightAboveGround', 'meanSea'], 'filter_by_keys': {'stream': ['oper']}, 'tag': 'stream-oper'}, {'time_dims': ['valid_time'], 'ignore_keys': ['edition'], 'extra_coords': {'expver': 'valid_time'}, 'coords_as_attributes': ['surface', 'depthBelowLandLayer', 'entireAtmosphere', 'heightAboveGround', 'meanSea'], 'filter_by_keys': {'stream': ['wave']}, 'tag': 'stream-wave'}]\n",
      "  post_open_datasets_kwargs: {'rename': {'time': 'forecast_reference_time', 'step': 'forecast_period', 'isobaricInhPa': 'pressure_level', 'hybrid': 'model_level'}, 'expand_dims': ['valid_time', 'pressure_level', 'model_level']}\n",
      "  to_netcdf_kwargs: {'engine': 'h5netcdf'}\n",
      "  compression_options: {'zlib': True, 'complevel': 1, 'shuffle': True}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Configuration options for opening the GRIB files as an xarray object,\n",
    "#  these depend on the dataset your are working with, this example is for ERA5 single-levels\n",
    "\n",
    "# The open_datasets_kwargs is a list of dictionaries, which is used to open the grib file into a list of\n",
    "#  consistent hypercube which are compatible netCDF. There are a number of common elements, so we define a\n",
    "#  common set and use them in each open_dataset_kwargs dictionary.\n",
    "common_kwargs = {\n",
    "    'time_dims': ['valid_time'],\n",
    "    'ignore_keys': ['edition'],\n",
    "    'extra_coords': {'expver': 'valid_time'},\n",
    "    'coords_as_attributes': [\n",
    "        'surface',\n",
    "        'depthBelowLandLayer',\n",
    "        'entireAtmosphere',\n",
    "        'heightAboveGround',\n",
    "        'meanSea'\n",
    "    ]\n",
    "}\n",
    "open_datasets_kwargs: dict[str, Any] | list[dict[str, Any]] = [\n",
    "    # To open the atmospheric variables\n",
    "    {**common_kwargs, 'filter_by_keys': {'stream': ['oper']}, 'tag': 'stream-oper'},\n",
    "    # To open the ocean-wave variables\n",
    "    {**common_kwargs,'filter_by_keys': {'stream': ['wave']}, 'tag': 'stream-wave'}\n",
    "]\n",
    "# Keywords that are used to modify the xarray object after opening\n",
    "post_open_datasets_kwargs: dict[str, Any] = {\n",
    "    'rename': {\n",
    "        'time': 'forecast_reference_time',\n",
    "        'step': 'forecast_period',\n",
    "        'isobaricInhPa': 'pressure_level',\n",
    "        'hybrid': 'model_level'\n",
    "    },\n",
    "    'expand_dims': ['valid_time', 'pressure_level', 'model_level']\n",
    "}\n",
    "\n",
    "# Configuration options for writing the xarray object to a netcdf file,\n",
    "#  These are the options used for all datasets in the CADS\n",
    "# Keywords that are used when writing to netcdf\n",
    "to_netcdf_kwargs: dict[str, Any] = {\n",
    "    \"engine\": \"h5netcdf\",\n",
    "}\n",
    "# Compression options to use when writing to netcdf, note that they are dependent on the engine\n",
    "compression_options = {\n",
    "    \"zlib\": True,\n",
    "    \"complevel\": 1,\n",
    "    \"shuffle\": True,\n",
    "}\n",
    "\n",
    "print(\n",
    "    f\"Converting {grib_file} to netCDF files with:\\n\"\n",
    "    f\"  open_datasets_kwargs: {open_datasets_kwargs}\\n\"\n",
    "    f\"  post_open_datasets_kwargs: {post_open_datasets_kwargs}\\n\"\n",
    "    f\"  to_netcdf_kwargs: {to_netcdf_kwargs}\\n\"\n",
    "    f\"  compression_options: {compression_options}\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open the GRIB as a dictionary of xarray datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is handled differently depending on whether the kwargs are passed as a list of dictionaries, or a single dictionary.\n",
    "\n",
    "If a list we iterate over each dictionary and return a dataset which matches it. These should include the filter_by_keys argument which will only select the fields which match the filter. This is used to split the grib file into complete hypercubes (which are compatible with NetCDF). We can then use the tag argument to label each hypercube dataset, and subsequent NetCDF file, with an appropriate name.\n",
    "\n",
    "If a single dictionary, we first try to open directly with `xarray.open_dataset` as a single hypercube. If there is an inconsistency in the hypercube, then this will fail and we retry to open with `cfgrib.open_datasets` which will automatically split the grib into complete hypercubes in the form of a list of datasets. NOTE: One draw back here is that cfgrib does not provide the information as to how the grib file was split, so we cannot label the hypercubes appropriately, therefore we just label with an number. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if open_datasets_kwargs is a list, then we open the grib file as a list of datasets\n",
    "if isinstance(open_datasets_kwargs, list):\n",
    "    datasets: dict[str, xr.Dataset] = {}\n",
    "    for i, open_ds_kwargs in enumerate(open_datasets_kwargs):\n",
    "        # Default engine is cfgrib\n",
    "        open_ds_kwargs.setdefault(\"engine\", \"cfgrib\")\n",
    "        # The tag in the open_datasets_kwargs is used to name the dataset,\n",
    "        # and subsequently the NetCDF file, if no tag is provided the index number is used\n",
    "        ds_tag = open_ds_kwargs.pop(\"tag\", i)\n",
    "        try:\n",
    "            ds = xr.open_dataset(grib_file, **open_ds_kwargs)\n",
    "        except Exception:\n",
    "            ds = None\n",
    "        if ds:\n",
    "            datasets[f\"{fname}_{ds_tag}\"] = ds\n",
    "else:\n",
    "    # We make sure that cfgrib raises an error if it encounters any issues, \n",
    "    # e.g. with inconsistent field dimensions\n",
    "    open_datasets_kwargs.setdefault(\"errors\", \"raise\")\n",
    "\n",
    "    # First try and open with xarray as a single dataset,\n",
    "    try:\n",
    "        datasets = {\n",
    "            f\"{fname}\": xr.open_dataset(grib_file, **open_datasets_kwargs)\n",
    "        }\n",
    "    except Exception:\n",
    "        print(\n",
    "            f\"Failed to open with xr.open_dataset({grib_file}, **{open_datasets_kwargs}), \"\n",
    "            \"opening with cfgrib.open_datasets instead.\"\n",
    "        )\n",
    "        # cfgrib.open_datasets returns a list of datasets, and it is not yet possible to sensibly tag them,\n",
    "        # so we use the index number as the tag\n",
    "        datasets = {\n",
    "            f\"{fname}_{i}\": ds\n",
    "            for i, ds in enumerate(\n",
    "                cfgrib.open_datasets(grib_file, **open_datasets_kwargs)\n",
    "            )\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modify the xarray object to meet the defined standard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw output from cfgrib uses the native MARS-GRIB metadata keys for the dimensions and coordinates. We modify this such that the dimension and coordinate names are a little more user friendly, and easy to distinguish given overlaps with CF convention definitions.\n",
    "\n",
    "To make these steps easier to work with we have defined stand-alone functions, then apply them as we iterate over the dictionary of datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to safely rename variables in an xarray dataset,\n",
    "#  i.e. ensures that the new names are not already in the dataset\n",
    "def safely_rename_variable(dataset: xr.Dataset, rename: dict[str, str]) -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Rename variables in an xarray dataset,\n",
    "    ensuring that the new names are not already in the dataset.\n",
    "    \"\"\"\n",
    "    # Create a rename order based on variabels that exist in datasets, and if there is\n",
    "    # a conflict, the variable that is being renamed will be renamed first.\n",
    "    rename_order: list[str] = []\n",
    "    conflicts: list[str] = []\n",
    "    for old_name, new_name in rename.items():\n",
    "        if old_name not in dataset:\n",
    "            continue\n",
    "\n",
    "        if new_name in dataset:\n",
    "            rename_order.append(old_name)\n",
    "            conflicts.append(old_name)\n",
    "        else:\n",
    "            rename_order = [old_name] + rename_order\n",
    "\n",
    "    # Ensure that the conflicts are handled correctly\n",
    "    # Is this necessary? We can let xarray fail by itself in the next step.\n",
    "    for conflict in conflicts:\n",
    "        new_name = rename[conflict]\n",
    "        if (new_name not in rename_order) or (\n",
    "            rename_order.index(conflict) > rename_order.index(new_name)\n",
    "        ):\n",
    "            raise ValueError(\n",
    "                f\"Refusing to to rename to existing variable name: {conflict}->{new_name}\"\n",
    "            )\n",
    "\n",
    "    for old_name in rename_order:\n",
    "        dataset = dataset.rename({old_name: rename[old_name]})\n",
    "\n",
    "    return dataset\n",
    "\n",
    "# Define a function to safely expand dimensions in an xarray dataset,\n",
    "#  ensures that the data for the new dimensions are in the dataset\n",
    "def safely_expand_dims(dataset: xr.Dataset, expand_dims: list[str]) -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Expand dimensions in an xarray dataset, ensuring that the new dimensions are not already in the dataset\n",
    "    and that the order of dimensions is preserved.\n",
    "    \"\"\"\n",
    "    dims_required = [c for c in dataset.coords if c in expand_dims + list(dataset.dims)]\n",
    "    dims_missing = [\n",
    "        (c, i) for i, c in enumerate(dims_required) if c not in dataset.dims\n",
    "    ]\n",
    "    dataset = dataset.expand_dims(\n",
    "        dim=[x[0] for x in dims_missing], axis=[x[1] for x in dims_missing]\n",
    "    )\n",
    "    return dataset\n",
    "\n",
    "\n",
    "out_datasets: dict[str, xr.Dataset] = {}\n",
    "for out_fname_base, dataset in datasets.items():\n",
    "    dataset = safely_rename_variable(dataset, post_open_datasets_kwargs.get(\"rename\", {}))\n",
    "\n",
    "    dataset = safely_expand_dims(dataset, post_open_datasets_kwargs.get(\"expand_dims\", []))\n",
    "\n",
    "    out_datasets[out_fname_base] = dataset\n",
    "\n",
    "datasets = out_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the datasets to NetCDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We loop over each dataset in the dictionary and write to netCDF, using the specified options. Note that the compression options must be added to each variable in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NetCDF file(s) created:\n",
      " data_stream-oper.nc\n",
      "data_stream-wave.nc\n"
     ]
    }
   ],
   "source": [
    "# put the engine in the to_netcdf_kwargs, and remove it from the compression options\n",
    "to_netcdf_kwargs.setdefault(\"engine\", compression_options.pop(\"engine\", \"h5netcdf\"))\n",
    "out_nc_files = []\n",
    "for out_fname_base, dataset in datasets.items():\n",
    "    to_netcdf_kwargs.update(\n",
    "        {\n",
    "            # Add the compression options to the encoding of each variable in the dataset\n",
    "            \"encoding\": {var: compression_options for var in dataset},\n",
    "        }\n",
    "    )\n",
    "    out_fname = f\"{out_fname_base}.nc\"\n",
    "    dataset.to_netcdf(out_fname, **to_netcdf_kwargs)\n",
    "    out_nc_files.append(out_fname)\n",
    "print(\"NetCDF file(s) created:\\n\", \"\\n\".join(out_nc_files))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CDS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
